import requests
from typing import Dict, Any, Tuple
import random
from pprint import pprint
from concurrent.futures import ThreadPoolExecutor, as_completed
import redis
import hashlib
import pickle
import uuid
import difflib
import re


def send_post_request(
    url: str, access_token: str, payload: Dict[str, Any], timeout: float = 60.0
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå¸¦è®¤è¯çš„JSON POSTè¯·æ±‚

    å‚æ•°ï¼š
    url (str): è¯·æ±‚åœ°å€
    access_token (str): Bearerä»¤ç‰Œ
    payload (dict): è¯·æ±‚ä½“æ•°æ®
    timeout (float): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    è¿”å›ï¼š
    dict: åŒ…å«çŠ¶æ€ç ã€å“åº”å¤´å’Œå“åº”æ•°æ®çš„å­—å…¸

    å¼‚å¸¸ï¼š
    requests.exceptions.RequestException: å°è£…æ‰€æœ‰è¯·æ±‚å¼‚å¸¸
    """
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json",
    }

    try:
        # å‘é€è¯·æ±‚å¹¶è‡ªåŠ¨åºåˆ—åŒ–JSON
        response = requests.post(
            url, headers=headers, json=payload, timeout=timeout  # è‡ªåŠ¨å¤„ç†JSONåºåˆ—åŒ–
        )

        # æ£€æŸ¥HTTPçŠ¶æ€ç 
        response.raise_for_status()

        # æ™ºèƒ½è§£æå“åº”å†…å®¹
        try:
            response_data = response.json()
        except ValueError:
            response_data = response.text

        return {
            "status_code": response.status_code,
            "headers": dict(response.headers),
            "data": response_data,
        }

    except requests.exceptions.RequestException as e:
        error_info = {
            "error_type": type(e).__name__,
            "status_code": getattr(e.response, "status_code", None),
            "error_message": str(e),
            "request_payload": payload,  # ä¿ç•™è¯·æ±‚æ•°æ®ç”¨äºè°ƒè¯•
        }
        raise requests.exceptions.RequestException(error_info) from e


# Redisé…ç½®ï¼Œä½¿ç”¨ä½ çš„æœåŠ¡åœ°å€å’Œç«¯å£
REDIS_HOST = '10.200.99.220'
REDIS_PORT = 32132
REDIS_DB = 0
REDIS_EXPIRE = 7 * 24 * 3600  # 7å¤©

redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB)


def make_cache_key(query, topk):
    key_str = f"{query}||{topk}"
    return "geogpt_cache:" + hashlib.md5(key_str.encode('utf-8')).hexdigest()


def geogpt_search(query, topk=2, DEBUG_GEOGPT_SEARCH=False, verbose=False):
    # é€‰æ‹©ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬æˆ–æ ‡å‡†ç‰ˆæœ¬çš„æ¨¡ç³Šæœç´¢ç¼“å­˜
    if USE_OPTIMIZED_FUZZY_SEARCH:
        cached_result, cache_status = fuzzy_search_cache_optimized(query, topk, redis_client, verbose)
    else:
        cached_result, cache_status = fuzzy_search_cache(query, topk, redis_client, verbose)
    if cached_result is not None:
        if DEBUG_GEOGPT_SEARCH and verbose:
            print(f"[DEBUG] {cache_status} for: {query} topk={topk}")
        return cached_result, cache_status
    try:
        request_payload = {
            "session_id": str(uuid.uuid4()),  # æ¯æ¬¡ç”Ÿæˆéšæœºuuid
            "query": query,
            "rag_switch": "chat_common",
            "top_k": topk,
        }
        # TODO(wenxun): Only for test, remove it in production
        result = send_post_request(
            url="http://10.200.48.226:30773/plugin/rag/retrieve/geo_common",
            access_token="sk-K30WQxD91JJ4mQLHMM5Q",
            payload=request_payload,
        )
        if DEBUG_GEOGPT_SEARCH:
            if random.random() < 0.01:
                print(f"[DEBUG] The query is: {query}")
                pprint(result)
        cacheit = True
        data = result.get("data", {})
        if not isinstance(data, dict):
            data = {}
            cacheit = False
        final_list = data.get("final", [])
        all_page_contents = []
        for item in final_list:
            page_content = item.get("page_content", None)
            temp_metadata = item.get("metadata", {})
            title = temp_metadata.get("title", None)
            if page_content:
                all_page_contents.append(f"(Title: {title}) {page_content}\n")
        if DEBUG_GEOGPT_SEARCH and len(final_list) == 0 and verbose:
            print(f"[ERROR] Items without page_content for query '{query}':")
            print("Request payload:")
            pprint(request_payload)
            print("Response data:")
            pprint(result)
        format_reference = ""
        for i, content in enumerate(all_page_contents):
            format_reference += f"Doc {i + 1}{content}"
        # å†™å…¥ç¼“å­˜
        if cacheit:
            try:
                cache_key = make_cache_key(query, topk)
                redis_client.set(cache_key, pickle.dumps(format_reference), ex=REDIS_EXPIRE)
                # åŒæ—¶å­˜å‚¨æŸ¥è¯¢æ˜ å°„ï¼Œç”¨äºæ¨¡ç³Šæœç´¢
                store_query_mapping(query, topk, cache_key, redis_client)
            except Exception as e:
                print(f"[WARN] Redis set error: {e}")

        if format_reference:
            return format_reference, 'new'
        else:
            return f"Sorry, not found answer for question {query} in the knowledge base.", 'not_found'
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return "Sorry, Server unavailable. Please try again later.", 'timeout'
    except Exception as other_err:
        print(f"è¯·æ±‚å¤±è´¥: {other_err}")
        return "Sorry, something went wrong. Please try again later.", 'error'


def geogpt_batch_search(queries, topk=2, DEBUG_GEOGPT_SEARCH=True, max_workers=8, verbose=False):
    """
    å¹¶å‘æ‰¹é‡æœç´¢å‡½æ•°
    """
    nonsense_count = 0
    exact_hit_count = 0
    fuzzy_hit_count = 0
    timeout_count = 0
    new_count = 0
    not_found_count = 0
    error_count = 0
    results = [None] * len(queries)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {}
        for idx, query in enumerate(queries):
            if not query or query.strip() in ["", "...", "and", "query"]:
                if verbose:
                    print("[WARN] Asking for searching nonsense...")
                results[idx] = (
                    "Sorry, I don't think the question you asked between <search> and </search> is meaningful. Please try again."
                )
                nonsense_count += 1
            elif len(query) < 3:
                if verbose:
                    print("[WARN] Asking for searching nonsense, too short query...")
                results[idx] = (
                    "Sorry, I don't think the question you asked between <search> and </search> is meaningful (it is too short). Please try again."
                )
                nonsense_count += 1
            elif len(query) > 300:
                if verbose:
                    print("[WARN] Asking for searching nonsense, too long query...")
                results[idx] = (
                    "Sorry, I don't think the question you asked between <search> and </search> is meaningful (it is too long). Please try again."
                )
                nonsense_count += 1
            else:
                future = executor.submit(
                    geogpt_search, query, topk, False, verbose
                )
                future_to_idx[future] = idx
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                result, status = future.result()
                results[idx] = result
                if status == 'exact_hit':
                    exact_hit_count += 1
                elif status == 'fuzzy_hit':
                    fuzzy_hit_count += 1
                elif status == 'new':
                    new_count += 1
                elif status == 'not_found':
                    not_found_count += 1
                elif status == 'timeout':
                    timeout_count += 1
                elif status == 'error':
                    error_count += 1
            except Exception as e:
                results[idx] = f"Error: {e}"
                error_count += 1
    if DEBUG_GEOGPT_SEARCH:
        len_queries = len(queries)
        len_results = len(results)
        total_cache_hits = exact_hit_count + fuzzy_hit_count
        print("="*50)
        print(f"Total queries: {len_queries}")
        print(f"Total results: {len_results}")
        print(f"Nonsense: {nonsense_count / len_queries * 100:.2f}%")
        print(f"Exact cache hits: {exact_hit_count / len_queries * 100:.2f}%")
        print(f"Fuzzy cache hits: {fuzzy_hit_count / len_queries * 100:.2f}%")
        print(f"Total cache hits: {total_cache_hits / len_queries * 100:.2f}%")
        print(f"Timeout/Error: {(timeout_count + error_count) / len_queries * 100:.2f}%")
        if new_count > 0:
            print(f"T/E Rate (Server side): {(timeout_count + error_count) / new_count * 100:.2f}%")
        print(f"Newly requested: {new_count / len_queries * 100:.2f}%")
        print(f"Not found: {not_found_count / len_queries * 100:.2f}%")
        if len_queries != len_results:
            print(f"[ERROR] Mismatch in queries and results length: {len_queries} vs {len_results}")
        print("="*50)
    return results


# æ¨¡ç³Šæœç´¢é…ç½®
FUZZY_SEARCH_ENABLED = True
FUZZY_SIMILARITY_THRESHOLD = 0.9  # ç›¸ä¼¼åº¦é˜ˆå€¼(0-1)
FUZZY_MAX_CANDIDATES = 200  # å¢åŠ å€™é€‰æ•°é‡ï¼Œæé«˜åŒ¹é…è´¨é‡
FUZZY_SCAN_COUNT = 100  # æ¯æ¬¡SCANæ“ä½œçš„æ•°é‡
USE_OPTIMIZED_FUZZY_SEARCH = True  # æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„æ¨¡ç³Šæœç´¢


def calculate_similarity(str1: str, str2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
    ä½¿ç”¨Pythonå†…ç½®çš„difflibåº“
    è¿”å›å€¼èŒƒå›´ 0-1ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒ
    """
    return difflib.SequenceMatcher(None, str1.lower(), str2.lower()).ratio()


def normalize_query(query: str) -> str:
    """
    æ ‡å‡†åŒ–æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…
    """
    # è½¬æ¢ä¸ºå°å†™
    query = query.lower()
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
    query = re.sub(r'\s+', ' ', query).strip()
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    # query = re.sub(r'[^\w\s]', '', query)
    return query


def fuzzy_search_cache(query: str, topk: int, redis_client, verbose: bool = False) -> Tuple[str, str]:
    """
    åœ¨Redisç¼“å­˜ä¸­æ‰§è¡Œæ¨¡ç³Šæœç´¢
    è¿”å›: (ç»“æœå†…å®¹, çŠ¶æ€)
    çŠ¶æ€å¯èƒ½æ˜¯: 'exact_hit', 'fuzzy_hit', 'not_found'
    """
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    exact_cache_key = make_cache_key(query, topk)
    try:
        cached = redis_client.get(exact_cache_key)
        if cached is not None:
            if verbose:
                print(f"[DEBUG] Exact cache hit for: {query}")
            return pickle.loads(cached), 'exact_hit'
    except Exception as e:
        if verbose:
            print(f"[WARN] Redis exact get error: {e}")
    
    # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ä¸”å¯ç”¨äº†æ¨¡ç³Šæœç´¢ï¼Œåˆ™è¿›è¡Œæ¨¡ç³Šæœç´¢
    if not FUZZY_SEARCH_ENABLED:
        return None, 'not_found'
    
    try:
        normalized_query = normalize_query(query)
        best_match_key = None
        best_similarity = 0
        
        # ä½¿ç”¨SCANä»£æ›¿KEYSï¼Œé¿å…é˜»å¡Redis
        # è·å–æ‰€æœ‰æŸ¥è¯¢æ˜ å°„çš„keysç”¨äºæ¨¡ç³Šæœç´¢
        query_keys = []
        cursor = 0
        query_to_key_pattern = "geogpt_query_map:*"
        
        while True:
            cursor, keys = redis_client.scan(cursor=cursor, match=query_to_key_pattern, count=100)
            query_keys.extend(keys)
            
            # é™åˆ¶æ£€æŸ¥çš„keyæ•°é‡ï¼Œé¿å…æ€§èƒ½é—®é¢˜
            if len(query_keys) >= FUZZY_MAX_CANDIDATES:
                query_keys = query_keys[:FUZZY_MAX_CANDIDATES]
                break
                
            if cursor == 0:
                break
        
        if not query_keys:
            return None, 'not_found'
        
        # å¦‚æœå€™é€‰é¡¹å¾ˆå¤šï¼Œä½¿ç”¨éšæœºé‡‡æ ·æé«˜è¦†ç›–ç‡
        if len(query_keys) > FUZZY_MAX_CANDIDATES:
            query_keys = random.sample(query_keys, FUZZY_MAX_CANDIDATES)
        
        for query_key in query_keys:
            try:
                stored_info = redis_client.get(query_key)
                if stored_info:
                    query_info = pickle.loads(stored_info)
                    stored_query = query_info.get('query', '')
                    stored_topk = query_info.get('topk', 0)
                    
                    # åªæ¯”è¾ƒç›¸åŒtopkçš„æŸ¥è¯¢
                    if stored_topk == topk:
                        normalized_stored = normalize_query(stored_query)
                        similarity = calculate_similarity(normalized_query, normalized_stored)
                        
                        if similarity > best_similarity and similarity >= FUZZY_SIMILARITY_THRESHOLD:
                            best_similarity = similarity
                            best_match_key = query_info.get('cache_key')
            except Exception as e:
                if verbose:
                    print(f"[WARN] Error processing query key {query_key}: {e}")
                continue
        
        # å¦‚æœæ‰¾åˆ°äº†è¶³å¤Ÿç›¸ä¼¼çš„æŸ¥è¯¢ï¼Œè¿”å›å…¶ç¼“å­˜ç»“æœ
        if best_match_key:
            try:
                cached_result = redis_client.get(best_match_key)
                if cached_result:
                    if verbose:
                        print(f"[DEBUG] Fuzzy cache hit for: {query} (similarity: {best_similarity:.3f})")
                    return pickle.loads(cached_result), 'fuzzy_hit'
            except Exception as e:
                if verbose:
                    print(f"[WARN] Error retrieving fuzzy match: {e}")
        
    except Exception as e:
        if verbose:
            print(f"[WARN] Fuzzy search error: {e}")
    
    return None, 'not_found'


def fuzzy_search_cache_optimized(query: str, topk: int, redis_client, verbose: bool = False) -> Tuple[str, str]:
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„æ¨¡ç³Šæœç´¢ï¼Œä½¿ç”¨Redis Sorted Setæé«˜æ€§èƒ½
    å½“æ•°æ®é‡å¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªç‰ˆæœ¬ä¼šæ›´é«˜æ•ˆ
    """
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    exact_cache_key = make_cache_key(query, topk)
    try:
        cached = redis_client.get(exact_cache_key)
        if cached is not None:
            if verbose:
                print(f"[DEBUG] Exact cache hit for: {query}")
            return pickle.loads(cached), 'exact_hit'
    except Exception as e:
        if verbose:
            print(f"[WARN] Redis exact get error: {e}")
    
    if not FUZZY_SEARCH_ENABLED:
        return None, 'not_found'
    
    try:
        normalized_query = normalize_query(query)
        
        # ä½¿ç”¨Redis Sorted Setå­˜å‚¨æŸ¥è¯¢çš„n-gramç‰¹å¾
        # è¿™æ ·å¯ä»¥æ›´å¿«åœ°æ‰¾åˆ°ç›¸ä¼¼çš„æŸ¥è¯¢
        query_ngrams = set(get_ngrams(normalized_query, 3))  # 3-gram
        
        best_match_key = None
        best_similarity = 0
        
        # ä½¿ç”¨pipelineæ‰¹é‡è·å–ï¼Œæé«˜æ€§èƒ½
        pipe = redis_client.pipeline()
        
        # åˆ†æ‰¹å¤„ç†æŸ¥è¯¢æ˜ å°„
        cursor = 0
        batch_size = 50
        processed_count = 0
        
        while processed_count < FUZZY_MAX_CANDIDATES:
            cursor, keys = redis_client.scan(
                cursor=cursor, 
                match="geogpt_query_map:*", 
                count=FUZZY_SCAN_COUNT
            )
            
            if not keys:
                if cursor == 0:
                    break
                continue
            
            # æ‰¹é‡è·å–æŸ¥è¯¢ä¿¡æ¯
            for key in keys[:min(batch_size, FUZZY_MAX_CANDIDATES - processed_count)]:
                pipe.get(key)
            
            results = pipe.execute()
            pipe.reset()
            
            # å¤„ç†è¿™ä¸€æ‰¹çš„ç»“æœ
            for i, stored_info in enumerate(results):
                if stored_info and processed_count < FUZZY_MAX_CANDIDATES:
                    try:
                        query_info = pickle.loads(stored_info)
                        stored_query = query_info.get('query', '')
                        stored_topk = query_info.get('topk', 0)
                        
                        if stored_topk == topk:
                            normalized_stored = normalize_query(stored_query)
                            
                            # å¿«é€Ÿé¢„ç­›é€‰ï¼šä½¿ç”¨n-gramé‡å åº¦
                            stored_ngrams = set(get_ngrams(normalized_stored, 3))
                            if query_ngrams and stored_ngrams:
                                ngram_overlap = len(query_ngrams & stored_ngrams) / len(query_ngrams | stored_ngrams)
                                # åªæœ‰n-gramé‡å åº¦è¶³å¤Ÿé«˜æ‰è¿›è¡Œè¯¦ç»†ç›¸ä¼¼åº¦è®¡ç®—
                                if ngram_overlap >= 0.3:
                                    similarity = calculate_similarity(normalized_query, normalized_stored)
                                    if similarity > best_similarity and similarity >= FUZZY_SIMILARITY_THRESHOLD:
                                        best_similarity = similarity
                                        best_match_key = query_info.get('cache_key')
                    except Exception as e:
                        if verbose:
                            print(f"[WARN] Error processing query info: {e}")
                    
                    processed_count += 1
            
            if cursor == 0:
                break
        
        # å¦‚æœæ‰¾åˆ°äº†è¶³å¤Ÿç›¸ä¼¼çš„æŸ¥è¯¢ï¼Œè¿”å›å…¶ç¼“å­˜ç»“æœ
        if best_match_key:
            try:
                cached_result = redis_client.get(best_match_key)
                if cached_result:
                    if verbose:
                        print(f"[DEBUG] Optimized fuzzy cache hit for: {query} (similarity: {best_similarity:.3f})")
                    return pickle.loads(cached_result), 'fuzzy_hit'
            except Exception as e:
                if verbose:
                    print(f"[WARN] Error retrieving fuzzy match: {e}")
        
    except Exception as e:
        if verbose:
            print(f"[WARN] Optimized fuzzy search error: {e}")
    
    return None, 'not_found'


def get_ngrams(text: str, n: int = 3):
    """
    ç”Ÿæˆæ–‡æœ¬çš„n-gramç‰¹å¾
    ç”¨äºå¿«é€Ÿç›¸ä¼¼åº¦é¢„ç­›é€‰
    """
    if len(text) < n:
        return [text]
    return [text[i:i+n] for i in range(len(text) - n + 1)]


def store_query_mapping(query: str, topk: int, cache_key: str, redis_client):
    """
    å­˜å‚¨æŸ¥è¯¢åˆ°ç¼“å­˜keyçš„æ˜ å°„ï¼Œç”¨äºåç»­çš„æ¨¡ç³Šæœç´¢
    """
    try:
        mapping_key = "geogpt_query_map:" + hashlib.md5(f"{query}||{topk}".encode('utf-8')).hexdigest()
        query_info = {
            'query': query,
            'topk': topk,
            'cache_key': cache_key
        }
        redis_client.set(mapping_key, pickle.dumps(query_info), ex=REDIS_EXPIRE)
    except Exception as e:
        print(f"[WARN] Error storing query mapping: {e}")


def test_fuzzy_search():
    """
    æµ‹è¯•æ¨¡ç³Šæœç´¢åŠŸèƒ½çš„ç¤ºä¾‹å‡½æ•°
    """
    print("=== æ¨¡ç³Šæœç´¢åŠŸèƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
    test_queries = [
        ("ä¸­å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ", "ä¸­å›½é¦–éƒ½æ˜¯ä»€ä¹ˆ"),
        ("What is the capital of China?", "What's the capital of China"),
        ("How to make a cake?", "å¦‚ä½•åˆ¶ä½œè›‹ç³•ï¼Ÿ"),
        ("Google presentation end screen", "Google presentation end screen mark")
    ]
    
    print("\nç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•:")
    for q1, q2 in test_queries:
        similarity = calculate_similarity(q1, q2)
        print(f"'{q1}' vs '{q2}': {similarity:.3f}")
    
    print("\nå½“å‰æ¨¡ç³Šæœç´¢é…ç½®:")
    print(f"- å¯ç”¨çŠ¶æ€: {FUZZY_SEARCH_ENABLED}")
    print(f"- ç›¸ä¼¼åº¦é˜ˆå€¼: {FUZZY_SIMILARITY_THRESHOLD}")
    print(f"- æœ€å¤§å€™é€‰æ•°é‡: {FUZZY_MAX_CANDIDATES}")
    print(f"- SCANæ‰¹æ¬¡å¤§å°: {FUZZY_SCAN_COUNT}")
    print(f"- ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬: {USE_OPTIMIZED_FUZZY_SEARCH}")
    
    # æµ‹è¯•n-gramç‰¹å¾æå–
    print("\nn-gramç‰¹å¾æµ‹è¯•:")
    test_text = "How to make a cake"
    ngrams = get_ngrams(test_text, 3)
    print(f"æ–‡æœ¬: '{test_text}'")
    print(f"3-grams: {ngrams[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é¦–å…ˆæµ‹è¯•æ¨¡ç³Šæœç´¢åŠŸèƒ½
    test_fuzzy_search()
    
    print("\n" + "="*50)
    print("å¼€å§‹å®é™…æœç´¢æµ‹è¯•...")
    
    queries = [
        "ä¸­å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
        "What is the capital of China?",
        "How to make a cake?",
        "Google presentation end screen mark",
        "When did the first Â£1 coin come out?",
        "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "What are the benefits of machine learning?",
        "å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿ",
        "Best practices for database design",
        "JavaScriptå¼‚æ­¥ç¼–ç¨‹æ€ä¹ˆåšï¼Ÿ",
        "How to configure Docker containers?",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿ï¼Ÿ",
        "React vs Vue.js comparison",
        "å¦‚ä½•å­¦ä¹ æ•°æ®ç»“æ„å’Œç®—æ³•ï¼Ÿ",
        "Cloud computing security concerns",
        "MySQLæ•°æ®åº“ä¼˜åŒ–æŠ€å·§",
        "How to implement REST API?",
        "ç§»åŠ¨ç«¯å¼€å‘æ¡†æ¶é€‰æ‹©",
        "Git version control best practices",
        "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæŠ€å·§",
        "How to deploy applications to AWS?",
        "åŒºå—é“¾æŠ€æœ¯çš„åº”ç”¨åœºæ™¯",
        "Microservices architecture patterns",
        "å¦‚ä½•è¿›è¡Œä»£ç é‡æ„ï¼Ÿ",
        "Kubernetes cluster management"
    ]
    results = geogpt_batch_search(queries, topk=3)
    for i, query in enumerate(queries):
        print(f"Query: {query}")
        print(f"Result: {results[i][:100]}")
        print("=" * 50)
        
    # æµ‹è¯•æ¨¡ç³ŠåŒ¹é… - è¿è¡Œç›¸ä¼¼çš„æŸ¥è¯¢
    print("\n" + "="*50)
    print("æ¨¡ç³ŠåŒ¹é…æµ‹è¯• - ä½¿ç”¨ç›¸ä¼¼ä½†ä¸å®Œå…¨ç›¸åŒçš„æŸ¥è¯¢...")
    
    similar_queries = [
        "ä¸­å›½é¦–éƒ½æ˜¯ä»€ä¹ˆ",  # ä¸ç¬¬ä¸€ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "What's the capital of China",  # ä¸ç¬¬äºŒä¸ªæŸ¥è¯¢ç›¸ä¼¼  
        "How can I make a cake",  # ä¸ç¬¬ä¸‰ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "Pythonç¼–ç¨‹æœ‰ä»€ä¹ˆç‰¹ç‚¹",  # ä¸Pythonç‰¹ç‚¹æŸ¥è¯¢ç›¸ä¼¼
        "What are machine learning advantages?",  # ä¸æœºå™¨å­¦ä¹ å¥½å¤„æŸ¥è¯¢ç›¸ä¼¼
        "æ€æ ·æé«˜å·¥ä½œæ•ˆç‡",  # ä¸æé«˜å·¥ä½œæ•ˆç‡æŸ¥è¯¢ç›¸ä¼¼
        "Database design best practices",  # ä¸æ•°æ®åº“è®¾è®¡æŸ¥è¯¢ç›¸ä¼¼
        "JavaScriptå¼‚æ­¥ç¼–ç¨‹æ–¹æ³•",  # ä¸JSå¼‚æ­¥ç¼–ç¨‹æŸ¥è¯¢ç›¸ä¼¼
        "How to setup Docker containers?",  # ä¸Dockeré…ç½®æŸ¥è¯¢ç›¸ä¼¼
        "AIå‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆ",  # ä¸AIå‘å±•è¶‹åŠ¿æŸ¥è¯¢ç›¸ä¼¼
        "Reactå’ŒVue.jså¯¹æ¯”",  # ä¸React vs VueæŸ¥è¯¢ç›¸ä¼¼
        "æ€ä¹ˆå­¦æ•°æ®ç»“æ„ç®—æ³•",  # ä¸å­¦ä¹ æ•°æ®ç»“æ„ç®—æ³•æŸ¥è¯¢ç›¸ä¼¼
        "Cloud security concerns",  # ä¸äº‘è®¡ç®—å®‰å…¨æŸ¥è¯¢ç›¸ä¼¼
        "MySQLä¼˜åŒ–æ–¹æ³•",  # ä¸MySQLä¼˜åŒ–æŸ¥è¯¢ç›¸ä¼¼
        "How to build REST API?",  # ä¸REST APIå®ç°æŸ¥è¯¢ç›¸ä¼¼
        "ç§»åŠ¨å¼€å‘æ¡†æ¶æ€ä¹ˆé€‰",  # ä¸ç§»åŠ¨ç«¯æ¡†æ¶é€‰æ‹©æŸ¥è¯¢ç›¸ä¼¼
        "Git best practices",  # ä¸Gitæœ€ä½³å®è·µæŸ¥è¯¢ç›¸ä¼¼
        "æ·±åº¦å­¦ä¹ è®­ç»ƒæ–¹æ³•",  # ä¸æ·±åº¦å­¦ä¹ è®­ç»ƒæŸ¥è¯¢ç›¸ä¼¼
        "How to deploy to AWS?",  # ä¸AWSéƒ¨ç½²æŸ¥è¯¢ç›¸ä¼¼
        "åŒºå—é“¾åº”ç”¨æœ‰å“ªäº›",  # ä¸åŒºå—é“¾åº”ç”¨æŸ¥è¯¢ç›¸ä¼¼
        "Microservices patterns",  # ä¸å¾®æœåŠ¡æ¶æ„æŸ¥è¯¢ç›¸ä¼¼
        "ä»£ç é‡æ„æ€ä¹ˆåš",  # ä¸ä»£ç é‡æ„æŸ¥è¯¢ç›¸ä¼¼
        "Kubernetes management"  # ä¸Kubernetesç®¡ç†æŸ¥è¯¢ç›¸ä¼¼
    ]
    
    similar_results = geogpt_batch_search(similar_queries, topk=3, verbose=True)
    for i, query in enumerate(similar_queries):
        print(f"Similar Query: {query}")
        print(f"Result: {similar_results[i][:100]}")
        print("=" * 50)
    
    # æµ‹è¯•è¾¹ç•Œæƒ…å†µå’Œç‰¹æ®ŠæŸ¥è¯¢
    print("\n" + "="*50)
    print("è¾¹ç•Œæƒ…å†µæµ‹è¯•...")
    
    edge_case_queries = [
        "",  # ç©ºæŸ¥è¯¢
        "   ",  # åªæœ‰ç©ºæ ¼
        "a",  # å•å­—ç¬¦
        "What?",  # éå¸¸çŸ­çš„æŸ¥è¯¢
        "è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸é•¿çš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç”¨æ¥æµ‹è¯•ç³»ç»Ÿåœ¨å¤„ç†è¶…é•¿æŸ¥è¯¢æ—¶çš„æ€§èƒ½è¡¨ç°å’Œç¨³å®šæ€§å¦‚ä½•",  # è¶…é•¿æŸ¥è¯¢
        "ğŸ¤”ğŸ”ğŸ’¡",  # emojiæŸ¥è¯¢
        "SELECT * FROM users WHERE id = 1;",  # SQLæŸ¥è¯¢
        "https://www.example.com/api/v1/users",  # URLæŸ¥è¯¢
        "2024-12-31 23:59:59",  # æ—¶é—´æ ¼å¼
        "user@example.com",  # é‚®ç®±æ ¼å¼
    ]
    
    edge_results = geogpt_batch_search(edge_case_queries, topk=3, verbose=True)
    for i, query in enumerate(edge_case_queries):
        print(f"Edge Case Query: '{query}'")
        print(f"Result: {edge_results[i][:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
        print("-" * 30)
    
    # æ€§èƒ½å‹åŠ›æµ‹è¯•
    print("\n" + "="*50)
    print("æ€§èƒ½å‹åŠ›æµ‹è¯• - å¤§æ‰¹é‡æŸ¥è¯¢...")
    
    import time
    
    # ç”Ÿæˆå¤§é‡é‡å¤å’Œç›¸ä¼¼çš„æŸ¥è¯¢æ¥æµ‹è¯•ç¼“å­˜æ•ˆæœ
    stress_queries = []
    base_queries = [
        "What is artificial intelligence?",
        "How to learn programming?",  
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æ•°æ®åº“è®¾è®¡åŸåˆ™",
        "Cloud computing benefits"
    ]
    
    # ä¸ºæ¯ä¸ªåŸºç¡€æŸ¥è¯¢ç”Ÿæˆå¤šä¸ªå˜ä½“
    variations = [
        "",
        " ",
        "?",
        "ï¼Ÿ",
        " please",
        " exactly",
        "can you tell me ",
        "I want to know ",
        "è¯·å‘Šè¯‰æˆ‘",
        "æˆ‘æƒ³äº†è§£"
    ]
    
    for base_query in base_queries:
        stress_queries.append(base_query)  # åŸæŸ¥è¯¢
        for variation in variations:
            if base_query.startswith("ä»€ä¹ˆ") or base_query.startswith("æ•°æ®"):
                # ä¸­æ–‡æŸ¥è¯¢åŠ ä¸­æ–‡å˜ä½“
                if variation in ["è¯·å‘Šè¯‰æˆ‘", "æˆ‘æƒ³äº†è§£"]:
                    stress_queries.append(variation + base_query)
            else:
                # è‹±æ–‡æŸ¥è¯¢åŠ è‹±æ–‡å˜ä½“
                if variation in ["can you tell me ", "I want to know "]:
                    stress_queries.append(variation + base_query)
                else:
                    stress_queries.append(base_query + variation)
    
    print(f"ç”Ÿæˆäº† {len(stress_queries)} ä¸ªæµ‹è¯•æŸ¥è¯¢")
    
    # æ‰§è¡Œå‹åŠ›æµ‹è¯•
    start_time = time.time()
    stress_results = geogpt_batch_search(stress_queries, topk=3, max_workers=16, verbose=False)
    end_time = time.time()
    
    print(f"å‹åŠ›æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ªæŸ¥è¯¢è€—æ—¶: {(end_time - start_time) / len(stress_queries):.3f}ç§’")
    print(f"QPS (æ¯ç§’æŸ¥è¯¢æ•°): {len(stress_queries) / (end_time - start_time):.2f}")
