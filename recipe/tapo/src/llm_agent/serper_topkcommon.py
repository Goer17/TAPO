import requests
from typing import Dict, Any, Tuple
import random
from pprint import pprint
from concurrent.futures import ThreadPoolExecutor, as_completed
import redis
import hashlib
import pickle
import difflib
import re
import time
import os


def send_post_request(
    url: str, headers: Dict[str, str], payload: Dict[str, Any], timeout: float = 60.0
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå¸¦è‡ªå®šä¹‰å¤´éƒ¨çš„JSON POSTè¯·æ±‚

    å‚æ•°ï¼š
    url (str): è¯·æ±‚åœ°å€
    headers (dict): è¯·æ±‚å¤´
    payload (dict): è¯·æ±‚ä½“æ•°æ®
    timeout (float): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    è¿”å›ï¼š
    dict: åŒ…å«çŠ¶æ€ç ã€å“åº”å¤´å’Œå“åº”æ•°æ®çš„å­—å…¸

    å¼‚å¸¸ï¼š
    requests.exceptions.RequestException: å°è£…æ‰€æœ‰è¯·æ±‚å¼‚å¸¸
    """
    try:
        # å‘é€è¯·æ±‚å¹¶è‡ªåŠ¨åºåˆ—åŒ–JSON
        response = requests.post(
            url, headers=headers, json=payload, timeout=timeout
        )

        # æ£€æŸ¥HTTPçŠ¶æ€ç 
        response.raise_for_status()

        # æ™ºèƒ½è§£æå“åº”å†…å®¹
        try:
            response_data = response.json()
        except ValueError:
            response_data = response.text

        return {
            "status_code": response.status_code,
            "headers": dict(response.headers),
            "data": response_data,
        }

    except requests.exceptions.RequestException as e:
        error_info = {
            "error_type": type(e).__name__,
            "status_code": getattr(e.response, "status_code", None),
            "error_message": str(e),
            "request_payload": payload,  # ä¿ç•™è¯·æ±‚æ•°æ®ç”¨äºè°ƒè¯•
        }
        raise requests.exceptions.RequestException(error_info) from e


# Redisé…ç½®ï¼Œä½¿ç”¨ä½ çš„æœåŠ¡åœ°å€å’Œç«¯å£
REDIS_HOST = os.environ.get('SEARCH_REDIS_HOST', '10.200.99.220')
REDIS_PORT = int(os.environ.get('SEARCH_REDIS_PORT', 32132))
REDIS_DB = 0
REDIS_EXPIRE = 7 * 24 * 3600  # 7å¤©

redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, socket_connect_timeout=10)

try:
    redis_client.ping()  # ä¸»åŠ¨æµ‹è¯•è¿æ¥
except redis.exceptions.ConnectionError as e:
    print(f"Redisè¿æ¥å¤±è´¥: {e}")

# Serper API
DEFAULT_SERPER_API_KEY = 'your_serper_api_key'
SERPER_API_URL = "https://google.serper.dev/search"

# æ¨¡ç³Šæœç´¢é…ç½®
FUZZY_SEARCH_ENABLED = True
FUZZY_SIMILARITY_THRESHOLD = 0.9  # ç›¸ä¼¼åº¦é˜ˆå€¼(0-1)
FUZZY_MAX_CANDIDATES = 200  # å¢åŠ å€™é€‰æ•°é‡ï¼Œæé«˜åŒ¹é…è´¨é‡
FUZZY_SCAN_COUNT = 100  # æ¯æ¬¡SCANæ“ä½œçš„æ•°é‡
USE_OPTIMIZED_FUZZY_SEARCH = True  # æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„æ¨¡ç³Šæœç´¢


def make_cache_key(query, topk):
    """ç”Ÿæˆç¼“å­˜key"""
    key_str = f"{query}||{topk}"
    return "serper_cache:" + hashlib.md5(key_str.encode('utf-8')).hexdigest()


def calculate_similarity(str1: str, str2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
    ä½¿ç”¨Pythonå†…ç½®çš„difflibåº“
    è¿”å›å€¼èŒƒå›´ 0-1ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒ
    """
    return difflib.SequenceMatcher(None, str1.lower(), str2.lower()).ratio()


def normalize_query(query: str) -> str:
    """
    æ ‡å‡†åŒ–æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…
    """
    # è½¬æ¢ä¸ºå°å†™
    query = query.lower()
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
    query = re.sub(r'\s+', ' ', query).strip()
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    # query = re.sub(r'[^\w\s]', '', query)
    return query


def get_ngrams(text: str, n: int = 3):
    """
    ç”Ÿæˆæ–‡æœ¬çš„n-gramç‰¹å¾
    ç”¨äºå¿«é€Ÿç›¸ä¼¼åº¦é¢„ç­›é€‰
    """
    if len(text) < n:
        return [text]
    return [text[i:i+n] for i in range(len(text) - n + 1)]


def store_query_mapping(query: str, topk: int, cache_key: str, redis_client):
    """
    å­˜å‚¨æŸ¥è¯¢åˆ°ç¼“å­˜keyçš„æ˜ å°„ï¼Œç”¨äºåç»­çš„æ¨¡ç³Šæœç´¢
    """
    try:
        mapping_key = "serper_query_map:" + hashlib.md5(f"{query}||{topk}".encode('utf-8')).hexdigest()
        query_info = {
            'query': query,
            'topk': topk,
            'cache_key': cache_key
        }
        redis_client.set(mapping_key, pickle.dumps(query_info), ex=REDIS_EXPIRE)
    except Exception as e:
        print(f"[WARN] Error storing query mapping: {e}")


def fuzzy_search_cache(query: str, topk: int, redis_client, verbose: bool = False) -> Tuple[str, str]:
    """
    åœ¨Redisç¼“å­˜ä¸­æ‰§è¡Œæ¨¡ç³Šæœç´¢
    è¿”å›: (ç»“æœå†…å®¹, çŠ¶æ€)
    çŠ¶æ€å¯èƒ½æ˜¯: 'exact_hit', 'fuzzy_hit', 'not_found'
    """
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    exact_cache_key = make_cache_key(query, topk)
    try:
        cached = redis_client.get(exact_cache_key)
        if cached is not None:
            if verbose:
                print(f"[DEBUG] Exact cache hit for: {query}")
            return pickle.loads(cached), 'exact_hit'
    except Exception as e:
        if verbose:
            print(f"[WARN] Redis exact get error: {e}")
    
    # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ä¸”å¯ç”¨äº†æ¨¡ç³Šæœç´¢ï¼Œåˆ™è¿›è¡Œæ¨¡ç³Šæœç´¢
    if not FUZZY_SEARCH_ENABLED:
        return None, 'not_found'
    
    try:
        normalized_query = normalize_query(query)
        best_match_key = None
        best_similarity = 0
        
        # ä½¿ç”¨SCANä»£æ›¿KEYSï¼Œé¿å…é˜»å¡Redis
        # è·å–æ‰€æœ‰æŸ¥è¯¢æ˜ å°„çš„keysç”¨äºæ¨¡ç³Šæœç´¢
        query_keys = []
        cursor = 0
        query_to_key_pattern = "serper_query_map:*"
        
        while True:
            cursor, keys = redis_client.scan(cursor=cursor, match=query_to_key_pattern, count=100)
            query_keys.extend(keys)
            
            # é™åˆ¶æ£€æŸ¥çš„keyæ•°é‡ï¼Œé¿å…æ€§èƒ½é—®é¢˜
            if len(query_keys) >= FUZZY_MAX_CANDIDATES:
                query_keys = query_keys[:FUZZY_MAX_CANDIDATES]
                break
                
            if cursor == 0:
                break
        
        if not query_keys:
            return None, 'not_found'
        
        # å¦‚æœå€™é€‰é¡¹å¾ˆå¤šï¼Œä½¿ç”¨éšæœºé‡‡æ ·æé«˜è¦†ç›–ç‡
        if len(query_keys) > FUZZY_MAX_CANDIDATES:
            query_keys = random.sample(query_keys, FUZZY_MAX_CANDIDATES)
        
        for query_key in query_keys:
            try:
                stored_info = redis_client.get(query_key)
                if stored_info:
                    query_info = pickle.loads(stored_info)
                    stored_query = query_info.get('query', '')
                    stored_topk = query_info.get('topk', 0)
                    
                    # åªæ¯”è¾ƒç›¸åŒtopkçš„æŸ¥è¯¢
                    if stored_topk == topk:
                        normalized_stored = normalize_query(stored_query)
                        similarity = calculate_similarity(normalized_query, normalized_stored)
                        
                        if similarity > best_similarity and similarity >= FUZZY_SIMILARITY_THRESHOLD:
                            best_similarity = similarity
                            best_match_key = query_info.get('cache_key')
            except Exception as e:
                if verbose:
                    print(f"[WARN] Error processing query key {query_key}: {e}")
                continue
        
        # å¦‚æœæ‰¾åˆ°äº†è¶³å¤Ÿç›¸ä¼¼çš„æŸ¥è¯¢ï¼Œè¿”å›å…¶ç¼“å­˜ç»“æœ
        if best_match_key:
            try:
                cached_result = redis_client.get(best_match_key)
                if cached_result:
                    if verbose:
                        print(f"[DEBUG] Fuzzy cache hit for: {query} (similarity: {best_similarity:.3f})")
                    return pickle.loads(cached_result), 'fuzzy_hit'
            except Exception as e:
                if verbose:
                    print(f"[WARN] Error retrieving fuzzy match: {e}")
        
    except Exception as e:
        if verbose:
            print(f"[WARN] Fuzzy search error: {e}")
    
    return None, 'not_found'


def fuzzy_search_cache_optimized(query: str, topk: int, redis_client, verbose: bool = False) -> Tuple[str, str]:
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„æ¨¡ç³Šæœç´¢ï¼Œä½¿ç”¨Redis Sorted Setæé«˜æ€§èƒ½
    å½“æ•°æ®é‡å¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªç‰ˆæœ¬ä¼šæ›´é«˜æ•ˆ
    """
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    exact_cache_key = make_cache_key(query, topk)
    try:
        cached = redis_client.get(exact_cache_key)
        if cached is not None:
            if verbose:
                print(f"[DEBUG] Exact cache hit for: {query}")
            return pickle.loads(cached), 'exact_hit'
    except Exception as e:
        if verbose:
            print(f"[WARN] Redis exact get error: {e}")
    
    if not FUZZY_SEARCH_ENABLED:
        return None, 'not_found'
    
    try:
        normalized_query = normalize_query(query)
        
        # ä½¿ç”¨Redis Sorted Setå­˜å‚¨æŸ¥è¯¢çš„n-gramç‰¹å¾
        # è¿™æ ·å¯ä»¥æ›´å¿«åœ°æ‰¾åˆ°ç›¸ä¼¼çš„æŸ¥è¯¢
        query_ngrams = set(get_ngrams(normalized_query, 3))  # 3-gram
        
        best_match_key = None
        best_similarity = 0
        
        # ä½¿ç”¨pipelineæ‰¹é‡è·å–ï¼Œæé«˜æ€§èƒ½
        pipe = redis_client.pipeline()
        
        # åˆ†æ‰¹å¤„ç†æŸ¥è¯¢æ˜ å°„
        cursor = 0
        batch_size = 50
        processed_count = 0
        
        while processed_count < FUZZY_MAX_CANDIDATES:
            cursor, keys = redis_client.scan(
                cursor=cursor, 
                match="serper_query_map:*", 
                count=FUZZY_SCAN_COUNT
            )
            
            if not keys:
                if cursor == 0:
                    break
                continue
            
            # æ‰¹é‡è·å–æŸ¥è¯¢ä¿¡æ¯
            for key in keys[:min(batch_size, FUZZY_MAX_CANDIDATES - processed_count)]:
                pipe.get(key)
            
            results = pipe.execute()
            pipe.reset()
            
            # å¤„ç†è¿™ä¸€æ‰¹çš„ç»“æœ
            for i, stored_info in enumerate(results):
                if stored_info and processed_count < FUZZY_MAX_CANDIDATES:
                    try:
                        query_info = pickle.loads(stored_info)
                        stored_query = query_info.get('query', '')
                        stored_topk = query_info.get('topk', 0)
                        
                        if stored_topk == topk:
                            normalized_stored = normalize_query(stored_query)
                            
                            # å¿«é€Ÿé¢„ç­›é€‰ï¼šä½¿ç”¨n-gramé‡å åº¦
                            stored_ngrams = set(get_ngrams(normalized_stored, 3))
                            if query_ngrams and stored_ngrams:
                                ngram_overlap = len(query_ngrams & stored_ngrams) / len(query_ngrams | stored_ngrams)
                                # åªæœ‰n-gramé‡å åº¦è¶³å¤Ÿé«˜æ‰è¿›è¡Œè¯¦ç»†ç›¸ä¼¼åº¦è®¡ç®—
                                if ngram_overlap >= 0.3:
                                    similarity = calculate_similarity(normalized_query, normalized_stored)
                                    if similarity > best_similarity and similarity >= FUZZY_SIMILARITY_THRESHOLD:
                                        best_similarity = similarity
                                        best_match_key = query_info.get('cache_key')
                    except Exception as e:
                        if verbose:
                            print(f"[WARN] Error processing query info: {e}")
                    
                    processed_count += 1
            
            if cursor == 0:
                break
        
        # å¦‚æœæ‰¾åˆ°äº†è¶³å¤Ÿç›¸ä¼¼çš„æŸ¥è¯¢ï¼Œè¿”å›å…¶ç¼“å­˜ç»“æœ
        if best_match_key:
            try:
                cached_result = redis_client.get(best_match_key)
                if cached_result:
                    if verbose:
                        print(f"[DEBUG] Optimized fuzzy cache hit for: {query} (similarity: {best_similarity:.3f})")
                    return pickle.loads(cached_result), 'fuzzy_hit'
            except Exception as e:
                if verbose:
                    print(f"[WARN] Error retrieving fuzzy match: {e}")
        
    except Exception as e:
        if verbose:
            print(f"[WARN] Optimized fuzzy search error: {e}")
    
    return None, 'not_found'


def serper_search(query: str, topk: int = 5, api_key: str = DEFAULT_SERPER_API_KEY, 
                 DEBUG_SERPER_SEARCH: bool = False, verbose: bool = False, timeout: float = 30.0) -> Tuple[str, str]:
    """
    ä½¿ç”¨Google Serper APIè¿›è¡Œæœç´¢ï¼Œå¸¦ç¼“å­˜å’Œæ¨¡ç³Šæœç´¢åŠŸèƒ½
    
    å‚æ•°ï¼š
    query (str): æœç´¢æŸ¥è¯¢
    topk (int): è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡
    api_key (str): Serper APIå¯†é’¥
    DEBUG_SERPER_SEARCH (bool): æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
    verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    timeout (float): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    è¿”å›ï¼š
    tuple: (æ ¼å¼åŒ–çš„æœç´¢ç»“æœ, çŠ¶æ€)
    çŠ¶æ€å¯èƒ½æ˜¯: 'exact_hit', 'fuzzy_hit', 'new', 'not_found', 'timeout', 'error'
    """
    # é€‰æ‹©ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬æˆ–æ ‡å‡†ç‰ˆæœ¬çš„æ¨¡ç³Šæœç´¢ç¼“å­˜
    if USE_OPTIMIZED_FUZZY_SEARCH:
        cached_result, cache_status = fuzzy_search_cache_optimized(query, topk, redis_client, verbose)
    else:
        cached_result, cache_status = fuzzy_search_cache(query, topk, redis_client, verbose)
    
    if cached_result is not None:
        if DEBUG_SERPER_SEARCH and verbose:
            print(f"[DEBUG] {cache_status} for: {query} topk={topk}")
        return cached_result, cache_status
    
    try:
        payload = {
            "q": query,
            "num": topk  # Serper APIä½¿ç”¨numå‚æ•°æ§åˆ¶ç»“æœæ•°é‡
        }
        
        headers = {
            'X-API-KEY': api_key,
            'Content-Type': 'application/json'
        }
        
        result = send_post_request(
            url=SERPER_API_URL,
            headers=headers,
            payload=payload,
            timeout=timeout
        )
        
        if DEBUG_SERPER_SEARCH:
            if random.random() < 0.01:
                print(f"[DEBUG] The query is: {query}")
                pprint(result)
        
        cacheit = True
        data = result.get("data", {})
        if not isinstance(data, dict):
            data = {}
            cacheit = False
        
        # æ ¼å¼åŒ–Serperæœç´¢ç»“æœ
        format_reference = format_serper_results_for_cache(data, topk)
        
        if DEBUG_SERPER_SEARCH and not format_reference and verbose:
            print(f"[ERROR] No results for query '{query}':")
            print("Request payload:")
            pprint(payload)
            print("Response data:")
            pprint(result)
        
        # å†™å…¥ç¼“å­˜
        if cacheit and format_reference:
            try:
                cache_key = make_cache_key(query, topk)
                redis_client.set(cache_key, pickle.dumps(format_reference), ex=REDIS_EXPIRE)
                # åŒæ—¶å­˜å‚¨æŸ¥è¯¢æ˜ å°„ï¼Œç”¨äºæ¨¡ç³Šæœç´¢
                store_query_mapping(query, topk, cache_key, redis_client)
            except Exception as e:
                print(f"[WARN] Redis set error: {e}")

        if format_reference:
            return format_reference, 'new'
        else:
            return f"Sorry, not found answer for question '{query}' in the search results.", 'not_found'
            
    except requests.exceptions.RequestException as e:
        print(f"æœç´¢è¯·æ±‚å¤±è´¥: {e}")
        return "Sorry, Search service unavailable. Please try again later.", 'timeout'
    except Exception as other_err:
        print(f"æœç´¢å¤±è´¥: {other_err}")
        return "Sorry, something went wrong with the search. Please try again later.", 'error'


def format_serper_results_for_cache(data: Dict[str, Any], max_results: int = 5) -> str:
    """
    æ ¼å¼åŒ–Serperæœç´¢ç»“æœä¸ºç¼“å­˜ç”¨çš„æ–‡æœ¬æ ¼å¼
    
    å‚æ•°ï¼š
    data (dict): Serper APIè¿”å›çš„æ•°æ®éƒ¨åˆ†
    max_results (int): æœ€å¤šè¿”å›çš„ç»“æœæ•°é‡
    
    è¿”å›ï¼š
    str: æ ¼å¼åŒ–åçš„æœç´¢ç»“æœæ–‡æœ¬
    """
    if not data:
        return ""
    
    formatted_results = []
    
    # æ·»åŠ çŸ¥è¯†é¢æ¿ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    knowledge_graph = data.get("knowledgeGraph", {})
    if knowledge_graph:
        title = knowledge_graph.get("title", "")
        description = knowledge_graph.get("description", "")
        if title and description:
            kg_info = f"(Knowledge Panel: {title}) {description}\n"
            formatted_results.append(kg_info)
    
    # å¤„ç†æœ‰æœºæœç´¢ç»“æœ
    organic_results = data.get("organic", [])
    for i, result in enumerate(organic_results[:max_results]):
        title = result.get("title", "No Title")
        snippet = result.get("snippet", "No Description")

        formatted_result = f"(Title: {title}) {snippet}\n"
        formatted_results.append(formatted_result)
    
    # ç»„åˆæ‰€æœ‰ç»“æœ
    final_reference = ""
    for i, content in enumerate(formatted_results):
        final_reference += f"Doc {i + 1}: {content}"
    
    return final_reference


def format_serper_results(search_result: Dict[str, Any], max_results: int = 5) -> str:
    """
    æ ¼å¼åŒ–Serperæœç´¢ç»“æœä¸ºå¯è¯»çš„æ–‡æœ¬æ ¼å¼ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    
    å‚æ•°ï¼š
    search_result (dict): Serper APIè¿”å›çš„å®Œæ•´æœç´¢ç»“æœ
    max_results (int): æœ€å¤šè¿”å›çš„ç»“æœæ•°é‡
    
    è¿”å›ï¼š
    str: æ ¼å¼åŒ–åçš„æœç´¢ç»“æœæ–‡æœ¬
    """
    if "error" in search_result:
        return f"æœç´¢å‡ºé”™: {search_result['error']}"
    
    data = search_result.get("data", {})
    if not data:
        return "æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœ"
    
    formatted_results = []
    
    # å¤„ç†æœ‰æœºæœç´¢ç»“æœ
    organic_results = data.get("organic", [])
    for i, result in enumerate(organic_results[:max_results]):
        title = result.get("title", "æ— æ ‡é¢˜")
        snippet = result.get("snippet", "æ— æè¿°")
        
        formatted_result = f"ç»“æœ {i + 1}:\n"
        formatted_result += f"æ ‡é¢˜: {title}\n"
        formatted_result += f"æè¿°: {snippet}\n"
        formatted_result += f"é“¾æ¥: {result.get('link', '')}\n"
        formatted_results.append(formatted_result)
    
    # æ·»åŠ çŸ¥è¯†é¢æ¿ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    knowledge_graph = data.get("knowledgeGraph", {})
    if knowledge_graph:
        title = knowledge_graph.get("title", "")
        description = knowledge_graph.get("description", "")
        if title and description:
            kg_info = f"\nçŸ¥è¯†é¢æ¿:\næ ‡é¢˜: {title}\næè¿°: {description}\n"
            formatted_results.insert(0, kg_info)
    
    # æ·»åŠ ç›¸å…³é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
    people_also_ask = data.get("peopleAlsoAsk", [])
    if people_also_ask:
        related_questions = "\nç›¸å…³é—®é¢˜:\n"
        for i, question in enumerate(people_also_ask[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            related_questions += f"{i + 1}. {question.get('question', '')}\n"
        formatted_results.append(related_questions)
    
    return "\n".join(formatted_results) if formatted_results else "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ"


def serper_batch_search(queries: list, topk: int = 5, api_key: str = DEFAULT_SERPER_API_KEY,
                       DEBUG_SERPER_SEARCH: bool = True, max_workers: int = 8, verbose: bool = False,
                       timeout: float = 30.0) -> list:
    """
    å¹¶å‘æ‰¹é‡Serperæœç´¢å‡½æ•°
    
    å‚æ•°ï¼š
    queries (list): æœç´¢æŸ¥è¯¢åˆ—è¡¨
    topk (int): æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡
    api_key (str): Serper APIå¯†é’¥
    DEBUG_SERPER_SEARCH (bool): æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
    max_workers (int): æœ€å¤§å¹¶å‘æ•°
    verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    timeout (float): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    è¿”å›ï¼š
    list: æœç´¢ç»“æœåˆ—è¡¨
    """
    nonsense_count = 0
    exact_hit_count = 0
    fuzzy_hit_count = 0
    timeout_count = 0
    new_count = 0
    not_found_count = 0
    error_count = 0
    results = [None] * len(queries)
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {}
        for idx, query in enumerate(queries):
            if not query or query.strip() in ["", "...", "and", "query"]:
                if verbose:
                    print("[WARN] Asking for searching nonsense...")
                results[idx] = (
                    "Sorry, I don't think the question you asked is meaningful. Please try again."
                )
                nonsense_count += 1
            elif len(query) < 3:
                if verbose:
                    print("[WARN] Asking for searching nonsense, too short query...")
                results[idx] = (
                    "Sorry, I don't think the question you asked is meaningful (it is too short). Please try again."
                )
                nonsense_count += 1
            elif len(query) > 300:
                if verbose:
                    print("[WARN] Asking for searching nonsense, too long query...")
                results[idx] = (
                    "Sorry, I don't think the question you asked is meaningful (it is too long). Please try again."
                )
                nonsense_count += 1
            else:
                future = executor.submit(
                    serper_search, query, topk, api_key, False, verbose, timeout
                )
                future_to_idx[future] = idx
        
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                result, status = future.result()
                results[idx] = result
                if status == 'exact_hit':
                    exact_hit_count += 1
                elif status == 'fuzzy_hit':
                    fuzzy_hit_count += 1
                elif status == 'new':
                    new_count += 1
                elif status == 'not_found':
                    not_found_count += 1
                elif status == 'timeout':
                    timeout_count += 1
                elif status == 'error':
                    error_count += 1
            except Exception as e:
                results[idx] = f"Error: {e}"
                error_count += 1
    
    if new_count > 0:
        try:
            redis_client.incrby('tcg:external_search_count', new_count)
        except Exception as e:
            print(f"[WARN] Error updating external search count in Redis: {e}")

    if DEBUG_SERPER_SEARCH:
        len_queries = len(queries)
        len_results = len(results)
        total_cache_hits = exact_hit_count + fuzzy_hit_count
        print("="*50)
        print(f"Total queries: {len_queries}")
        print(f"Total results: {len_results}")
        print(f"Nonsense: {nonsense_count / len_queries * 100:.2f}%")
        print(f"Exact cache hits: {exact_hit_count / len_queries * 100:.2f}%")
        print(f"Fuzzy cache hits: {fuzzy_hit_count / len_queries * 100:.2f}%")
        print(f"Total cache hits: {total_cache_hits / len_queries * 100:.2f}%")
        print(f"Timeout/Error: {(timeout_count + error_count) / len_queries * 100:.2f}%")
        if new_count > 0:
            print(f"T/E Rate (Server side): {(timeout_count + error_count) / new_count * 100:.2f}%")
        print(f"Newly requested: {new_count / len_queries * 100:.2f}%")
        print(f"Not found: {not_found_count / len_queries * 100:.2f}%")
        if len_queries != len_results:
            print(f"[ERROR] Mismatch in queries and results length: {len_queries} vs {len_results}")
        print("="*50)
    
    return results


def test_fuzzy_search():
    """
    æµ‹è¯•æ¨¡ç³Šæœç´¢åŠŸèƒ½çš„ç¤ºä¾‹å‡½æ•°
    """
    print("=== Serperæ¨¡ç³Šæœç´¢åŠŸèƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
    test_queries = [
        ("ä¸­å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ", "ä¸­å›½é¦–éƒ½æ˜¯ä»€ä¹ˆ"),
        ("What is the capital of China?", "What's the capital of China"),
        ("How to make a cake?", "å¦‚ä½•åˆ¶ä½œè›‹ç³•ï¼Ÿ"),
        ("Python programming tutorial", "Python programming guide")
    ]
    
    print("\nç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•:")
    for q1, q2 in test_queries:
        similarity = calculate_similarity(q1, q2)
        print(f"'{q1}' vs '{q2}': {similarity:.3f}")
    
    print("\nå½“å‰æ¨¡ç³Šæœç´¢é…ç½®:")
    print(f"- å¯ç”¨çŠ¶æ€: {FUZZY_SEARCH_ENABLED}")
    print(f"- ç›¸ä¼¼åº¦é˜ˆå€¼: {FUZZY_SIMILARITY_THRESHOLD}")
    print(f"- æœ€å¤§å€™é€‰æ•°é‡: {FUZZY_MAX_CANDIDATES}")
    print(f"- SCANæ‰¹æ¬¡å¤§å°: {FUZZY_SCAN_COUNT}")
    print(f"- ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬: {USE_OPTIMIZED_FUZZY_SEARCH}")
    
    # æµ‹è¯•n-gramç‰¹å¾æå–
    print("\nn-gramç‰¹å¾æµ‹è¯•:")
    test_text = "How to make a cake"
    ngrams = get_ngrams(test_text, 3)
    print(f"æ–‡æœ¬: '{test_text}'")
    print(f"3-grams: {ngrams[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª


def test_serper_search():
    """
    æµ‹è¯•Serperæœç´¢åŠŸèƒ½
    """
    print("=== Serperæœç´¢åŠŸèƒ½æµ‹è¯• ===")
    
    # å•ä¸ªæœç´¢æµ‹è¯•
    test_query = "apple inc"
    print(f"æµ‹è¯•æŸ¥è¯¢: {test_query}")
    
    try:
        result, status = serper_search(test_query, topk=3, verbose=True)
        print(f"æœç´¢æˆåŠŸ! çŠ¶æ€: {status}")
        print("æœç´¢ç»“æœ:")
        print(result[:500] + "..." if len(result) > 500 else result)
        
    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {e}")
    
    print("\n" + "="*50)
    
    # æ‰¹é‡æœç´¢æµ‹è¯•
    test_queries = [
        "python programming",
        "machine learning",
        "artificial intelligence",
        "web development",
        "data science"
    ]
    
    print("æ‰¹é‡æœç´¢æµ‹è¯•:")
    try:
        batch_results = serper_batch_search(test_queries, topk=3, max_workers=3, verbose=True)
        
        print("\næ‰¹é‡æœç´¢ç»“æœæ¦‚è§ˆ:")
        for i, (query, result) in enumerate(zip(test_queries, batch_results)):
            if isinstance(result, str) and not result.startswith("Error:"):
                print(f"{i+1}. {query}: æœç´¢æˆåŠŸ")
            else:
                print(f"{i+1}. {query}: æœç´¢å¤±è´¥ - {result}")
                
    except Exception as e:
        print(f"æ‰¹é‡æœç´¢å¤±è´¥: {e}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é¦–å…ˆæµ‹è¯•æ¨¡ç³Šæœç´¢åŠŸèƒ½
    test_fuzzy_search()
    
    print("\n" + "="*50)
    print("å¼€å§‹Serperæœç´¢æµ‹è¯•...")
    
    # åŸºç¡€æœç´¢æµ‹è¯•
    queries = [
        "What is artificial intelligence?",
        "How to learn Python programming?",
        "æœºå™¨å­¦ä¹ ç®—æ³•æœ‰å“ªäº›ï¼Ÿ",
        "Best web development frameworks 2025",
        "What are the benefits of cloud computing?",
        "How to optimize database performance?",
        "React vs Vue.js comparison",
        "ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯ï¼Ÿ",
        "Docker containerization tutorial",
        "How to deploy applications on AWS?"
    ]
    
    results = serper_batch_search(queries, topk=3, DEBUG_SERPER_SEARCH=True, verbose=False)
    for i, query in enumerate(queries):
        print(f"Query: {query}")
        # result_preview = results[i][:100] + "..." if len(results[i]) > 100 else results[i]
        result_preview = results[i]
        print(f"Result: {result_preview}")
        print("=" * 50)
        
    # æµ‹è¯•æ¨¡ç³ŠåŒ¹é… - è¿è¡Œç›¸ä¼¼çš„æŸ¥è¯¢
    print("\n" + "="*50)
    print("æ¨¡ç³ŠåŒ¹é…æµ‹è¯• - ä½¿ç”¨ç›¸ä¼¼ä½†ä¸å®Œå…¨ç›¸åŒçš„æŸ¥è¯¢...")
    
    similar_queries = [
        "What's artificial intelligence?",  # ä¸ç¬¬ä¸€ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "How can I learn Python programming",  # ä¸ç¬¬äºŒä¸ªæŸ¥è¯¢ç›¸ä¼¼  
        "æœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆç®—æ³•",  # ä¸ç¬¬ä¸‰ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "Best frameworks for web development",  # ä¸ç¬¬å››ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "What are cloud computing benefits?",  # ä¸ç¬¬äº”ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "How to improve database performance?",  # ä¸ç¬¬å…­ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "Reactå’ŒVue.jså¯¹æ¯”",  # ä¸ç¬¬ä¸ƒä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "åŒºå—é“¾æŠ€æœ¯æ˜¯ä»€ä¹ˆ",  # ä¸ç¬¬å…«ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "Docker container tutorial",  # ä¸ç¬¬ä¹ä¸ªæŸ¥è¯¢ç›¸ä¼¼
        "How to deploy apps to AWS?"  # ä¸ç¬¬åä¸ªæŸ¥è¯¢ç›¸ä¼¼
    ]
    
    similar_results = serper_batch_search(similar_queries, topk=3, verbose=True, DEBUG_SERPER_SEARCH=True)
    for i, query in enumerate(similar_queries):
        print(f"Similar Query: {query}")
        result_preview = similar_results[i][:100] + "..." if len(similar_results[i]) > 100 else similar_results[i]
        print(f"Result: {result_preview}")
        print("=" * 50)
    
    # æµ‹è¯•è¾¹ç•Œæƒ…å†µå’Œç‰¹æ®ŠæŸ¥è¯¢
    print("\n" + "="*50)
    print("è¾¹ç•Œæƒ…å†µæµ‹è¯•...")
    
    edge_case_queries = [
        "",  # ç©ºæŸ¥è¯¢
        "   ",  # åªæœ‰ç©ºæ ¼
        "a",  # å•å­—ç¬¦
        "What?",  # éå¸¸çŸ­çš„æŸ¥è¯¢
        "è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸é•¿çš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç”¨æ¥æµ‹è¯•ç³»ç»Ÿåœ¨å¤„ç†è¶…é•¿æŸ¥è¯¢æ—¶çš„æ€§èƒ½è¡¨ç°å’Œç¨³å®šæ€§ï¼Œçœ‹çœ‹æœç´¢å¼•æ“å¦‚ä½•å¤„ç†è¿™ç§æƒ…å†µ",  # è¶…é•¿æŸ¥è¯¢
        "ğŸ¤”ğŸ”ğŸ’¡",  # emojiæŸ¥è¯¢
        "python OR java",  # å¸ƒå°”æŸ¥è¯¢
        "\"exact phrase search\"",  # ç²¾ç¡®çŸ­è¯­æœç´¢
        "site:github.com python",  # ç«™ç‚¹é™åˆ¶æœç´¢
        "filetype:pdf machine learning"  # æ–‡ä»¶ç±»å‹æœç´¢
    ]
    
    edge_results = serper_batch_search(edge_case_queries, topk=3, verbose=True, DEBUG_SERPER_SEARCH=True)
    for i, query in enumerate(edge_case_queries):
        print(f"Edge Case Query: '{query}'")
        result_preview = edge_results[i][:100] + "..." if len(edge_results[i]) > 100 else edge_results[i]
        print(f"Result: {result_preview}")
        print("-" * 30)
    
    # æ€§èƒ½å‹åŠ›æµ‹è¯•
    print("\n" + "="*50)
    print("æ€§èƒ½å‹åŠ›æµ‹è¯• - å¤§æ‰¹é‡æŸ¥è¯¢...")
    
    # ç”Ÿæˆå¤§é‡é‡å¤å’Œç›¸ä¼¼çš„æŸ¥è¯¢æ¥æµ‹è¯•ç¼“å­˜æ•ˆæœ
    stress_queries = []
    base_queries = [
        "What is artificial intelligence?",
        "How to learn programming?",  
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æ•°æ®åº“è®¾è®¡åŸåˆ™",
        "Cloud computing benefits"
    ]
    
    # ä¸ºæ¯ä¸ªåŸºç¡€æŸ¥è¯¢ç”Ÿæˆå¤šä¸ªå˜ä½“
    variations = [
        "",
        " ",
        "?",
        "ï¼Ÿ",
        " 2025",
        " tutorial",
        "how to ",
        "what is ",
        "å¦‚ä½•",
        "ä»€ä¹ˆæ˜¯"
    ]
    
    for base_query in base_queries:
        stress_queries.append(base_query)  # åŸæŸ¥è¯¢
        for variation in variations:
            if base_query.startswith("ä»€ä¹ˆ") or base_query.startswith("æ•°æ®"):
                # ä¸­æ–‡æŸ¥è¯¢åŠ ä¸­æ–‡å˜ä½“
                if variation in ["å¦‚ä½•", "ä»€ä¹ˆæ˜¯"]:
                    stress_queries.append(variation + base_query[2:])  # å»æ‰åŸæœ‰çš„"ä»€ä¹ˆ"
            else:
                # è‹±æ–‡æŸ¥è¯¢åŠ è‹±æ–‡å˜ä½“
                if variation in ["how to ", "what is "]:
                    stress_queries.append(variation + base_query.lower())
                else:
                    stress_queries.append(base_query + variation)
    
    print(f"ç”Ÿæˆäº† {len(stress_queries)} ä¸ªæµ‹è¯•æŸ¥è¯¢")
    
    # æ‰§è¡Œå‹åŠ›æµ‹è¯•
    start_time = time.time()
    stress_results = serper_batch_search(stress_queries, topk=3, max_workers=16, verbose=False, DEBUG_SERPER_SEARCH=True)
    end_time = time.time()
    
    print(f"å‹åŠ›æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ªæŸ¥è¯¢è€—æ—¶: {(end_time - start_time) / len(stress_queries):.3f}ç§’")
    print(f"QPS (æ¯ç§’æŸ¥è¯¢æ•°): {len(stress_queries) / (end_time - start_time):.2f}")
